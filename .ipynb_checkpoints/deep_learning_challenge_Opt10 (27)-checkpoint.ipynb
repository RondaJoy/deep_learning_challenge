{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNyBHnbj0K987MW+mU1VGuA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0pkWGbScvo8e"},"outputs":[],"source":["# Import our dependencies\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","import tensorflow as tf\n","\n","#  Import and read the charity_data.csv.\n","import pandas as pd\n","application_df = pd.read_csv(\"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\")\n","application_df.head()"]},{"cell_type":"code","source":["# Drop the non-beneficial ID columns.\n","drop_columns = ['EIN', 'NAME', 'SPECIAL_CONSIDERATIONS', 'AFFILIATION', 'USE_CASE']\n","application_df = application_df.drop(drop_columns, 1)\n","application_df.head()"],"metadata":{"id":"MOdCuXfiv11r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Choose a cutoff value and create a list of application types to be replaced.  Use the variable name `application_types_to_replace`.\n","application_types_to_replace = ['T25', 'T14', 'T29', 'T15', 'T17']\n","\n","# Replace in dataframe\n","for app in application_types_to_replace:\n","    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n","\n","# Check to make sure binning was successful\n","application_df['APPLICATION_TYPE'].value_counts()"],"metadata":{"id":"ZbPHNMuCwdk3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Choose a cutoff value and create a list of classifications to be replaced. Use the variable name `classifications_to_replace`.\n","classification_counts = application_df['CLASSIFICATION'].value_counts()\n","classifications_to_replace = classification_counts[classification_counts <1000].index.tolist()\n","\n","# Replace in dataframe\n","for cls in classifications_to_replace:\n","    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n","\n","# Check to make sure binning was successful\n","application_df['CLASSIFICATION'].value_counts()"],"metadata":{"id":"DODcJQzKw0Mk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert categorical data to numeric with `pd.get_dummies`\n","application_encoded = pd.get_dummies(application_df)"],"metadata":{"id":"FPfs9lJ2xbFt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split our preprocessed data into our features and target arrays\n","X = application_encoded.drop('IS_SUCCESSFUL', axis=1)\n","y = application_encoded['IS_SUCCESSFUL']\n","\n","# Split the preprocessed data into a training and testing dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"],"metadata":{"id":"wanS0K13xg6x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a StandardScaler instances\n","scaler = StandardScaler()\n","\n","# Fit the StandardScaler\n","X_scaler = scaler.fit(X_train)\n","\n","# Scale the data\n","X_train_scaled = X_scaler.transform(X_train)\n","X_test_scaled = X_scaler.transform(X_test)"],"metadata":{"id":"yC2nOm1sxkbk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compile, Train and Evaluate the Model"],"metadata":{"id":"qGPTIpmXxrBx"}},{"cell_type":"code","source":["# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n","nfeatures = X_train.shape[1]\n","layer1 = 8\n","layer2 = 2\n","nn = tf.keras.models.Sequential()\n","\n","# First hidden layer\n","nn.add(tf.keras.layers.Dense(units=layer1, input_dim=nfeatures, activation=\"relu\"))\n","\n","# Second hidden layer\n","nn.add(tf.keras.layers.Dense(units=layer2, activation=\"relu\"))\n","\n","# Output layer\n","nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n","\n","# Check the structure of the model\n","nn.summary()"],"metadata":{"id":"eanT9XPPxoA0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compile the model\n","nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"],"metadata":{"id":"tzhVP7d-x1r7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","nn_trained = nn.fit(X_train_scaled,y_train,epochs=100)"],"metadata":{"id":"SFaTC5eSx4id"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model using the test data\n","model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n","print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"],"metadata":{"id":"lTMAzRUPyftg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Export your model to HDF5 file\n","nn.save(\"AlphabetSoupCharity_Optimization_02.h5\")"],"metadata":{"id":"RUojVK2Nyivc"},"execution_count":null,"outputs":[]}]}